\chapter{Forward Mode Differentiation} \label{sec:forwardMode}

Forward mode differentiation by hand is straight forward and also translates well into code by sticking to our existing knowledge about symbolic differentiation (i.e. differentiation ``by hand''). Remember the sum and product rule
\begin{align*}
    \text{Sum rule}&: (f + g)' = f' + g' \\
    \text{Product rule}&: (f \cdot g)' = f \cdot g' + g' \cdot f \\
\intertext{and how to differentiate the variable and constants:}
    \text{Variable rule}&: \diff{x}{x} = 1 \\
    \text{Constant rule}&: \diff{c}{x} = 0\ (c \neq x) \\
\end{align*}
These four differentiation rules build the base for our forward mode implementations. Essentially we want to go through every expression recursively and replace it with its derivative.


\section{Code replacement with macros} \label{sec:macros}
If you take the last sentence literally you could now go the long and hard way (which we did), learn metaprogramming and implement it exactly by replacing expressions with their derivative by directly applying the aforementioned differentiation rules. This approach looks rather ugly at first sight but after ignoring the boilerplate (with the help of the comments on the right-hand side) one can see that it just boils down to recursive pattern matching of code which works pretty intuitively in Scala 3~\cite{maScala3}:
\begin{lstlisting}
def d(t: Expr[Term])(using Quotes): Expr[Term] = t match
    case '{ ($l: Term) + ($r: Term) } =>    // l + r
        '{ ${ d(l) } + ${ d(r) } }          // d(l) + d(r)
    case '{ ($l: Term) * ($r: Term) } =>    // l * r
        '{ $l * ${ d(r) } + ${ d(l) } * $r }// l * d(r) + d(l) * r
    case '{ X } => '{ 1 }                   // variable
    case '{ $v: V } => '{ 0 }               // constant
\end{lstlisting}
 Ignore all types of line 1 and just think of \lstinline{t} as a (sub-)term (or expression for that matter) of a mathematical function we want to differentiate by calling \lstinline{d}. Now take for example line 2. We match \lstinline{t} to be a sum of two (sub-)terms (named \lstinline{l} and \lstinline{r}). In line 3 we have to define how \lstinline{t} should be replaced to get the derivative. For that we use the sum rule and essentially return \lstinline{d(l) + d(r)} which recursively applies the differentiation operator. The main challenge here is to ignore all those braces, dollar signs and apostrophes which are essentially a necessary evil to write macros but lets us intuitively pattern match over \emph{type checked} code which is a really powerful tool. Line 4 and 5 do the same but use the product rule. Line 6 matches the term to be \lstinline{X} which denotes our variable and is differentiated to 1. Analogously if a value is of type \lstinline{V} like in line 7 it is a constant which differentiates to 0.

In fact, we would not even need macros and could just write our term with algebraic data types and recursively match them at runtime to implement this approach. This would reduce the boilerplate in comparison to this macro-implementation.


\section{Operator overloading with dual numbers}\label{sec:forwardDualNumbers}

Rewriting code that way has one problem. We loose the actual result of our formula and only get the derivative (or we would have to calculate both separately). Consider we have the following function:
\begin{lstlisting}
def f(x: Double): Double = 2 * x + x * x * x
\end{lstlisting}
By writing \lstinline{f(3)} we can now calculate the result. Our goal is to simultaneously calculate the result and derivative of each subexpression to ultimately accumulate it into the final result and derivative. For this, we could use a structure that consists of two values. Such a construct is called a "dual number" and consequently has two members, one representing the value of an expression and the other representing the derivative of that expression:
\begin{lstlisting}
case class Dual(v: Double, d: Double)
\end{lstlisting}
To implement operations on dual numbers we have to define the actual computation and additionally how to compute the derivative:
\begin{lstlisting}[caption={Dual number implementation}, label={lst:forwardDualNumber}]
case class Dual(v: Double, d: Double):
    def *(that: Dual) = Dual(
      this.v * that.v,
      this.v * that.d + this.d * that.v // product rule
    )
  
    def +(that: Dual) = Dual(
      this.v + that.v,
      this.d + that.d // sum rule
    )
\end{lstlisting}
For multiplication this means that we can just multiply to get the actual result (line 3) and have to apply the product rule in line 4 by accessing \lstinline{v} to get the actual value and \lstinline{d} to get the derivative of the child expressions \lstinline{this} and \lstinline{that}. Addition works analogously but uses the sum rule.  As we can see, this comes down to translating mathematic symbolic differentiation rules into code. How to define constants and the variable (i.e. x) also comes naturally from the differentiation rules as the variable differentiates to 1 and constants to 0:
\begin{lstlisting}
def variable(v: Double) = Dual(v, 1)
def const(v: Double) = Dual(v, 0)
\end{lstlisting}
Differentiation of a function $f$ at $x = 3$ could then look like this:
\begin{lstlisting}[caption={Differentiation of a dual number function}, label={lst:forwardDualNumberDiff}]
def differentiate(f: Dual => Dual)(x: Double): Double =
    val result: Dual = f(variable(x))
    result.d // result.v would be the actual result of f(x)

def f(x: Dual): Dual = const(2) * x + x * x * x

val derivative = differentiate(f)(3)
\end{lstlisting}
The \lstinline{differentiate} function (line 1) essentially just applies a function to an argument for us (line 2) and then returns \lstinline{d} (i.e. the derivative) of the result (line 3).



\section{Match Types} \label{sec:matchTypes}
To explain the following approach we first have to look at match types which are essentially functions but at type level. Given a type, it uses patter-matching over that type to decide which type to return. It gets clear with the following example taken from the Scala 3 docs \cite{matchTypesScala3}:
\begin{lstlisting}
type Elem[X] = X match
    case String => Char
    case Array[t] => t
    case Iterable[t] => t
\end{lstlisting}
The match type \lstinline{Elem} has one argument \lstinline{X}. If \lstinline{X} is \lstinline{String} then it returns the type \lstinline{Char} (line 2). If \lstinline{X} is \lstinline{Array[t]} or \lstinline{Iterable[t]} it returns the generic type parameter of them, that is \lstinline{t}. In essence \lstinline{Elem} matches list-likes types and returns their element type. For example, it could be used like this:
\begin{lstlisting}
val i: Elem[Array[Int]] = 123
\end{lstlisting}
\lstinline{X} is in this case \lstinline{Array[Int]} and \lstinline{t} is therefore \lstinline{Int}. The type of \lstinline{i} ultimately reduces to \lstinline{Int}. By recursively matching types we can implement a differentiator match type \lstinline{D} purely at type level:
\begin{lstlisting}
type D[T <: Term] <: Term = T match
    case l * r => 
        l * D[r] + D[l] * r
    case l + r => 
        D[l] + D[r]
    case X => 
        V[1]
    case V[_] => 
        V[0]
\end{lstlisting}
\lstinline{D} itself and its argument \lstinline{T} are of type \lstinline{Term} which is just the super type of all of our expressions.
\lstinline{T} is a type but encodes a full calculation. Consider line 2 where we match \lstinline{T} to be \lstinline{l * r}. \lstinline{l} and \lstinline{r} are child \lstinline{Term}s (but are still types). In fact, the multiplication sign (\lstinline{*}) is  an infix type and not a method. It has two type parameters (in this case \lstinline{l} and \lstinline{r}) and is also a subtype of \lstinline{Term}:
\begin{lstlisting}
type *[L <: Term, R <: Term] <: Term
\end{lstlisting}
At this point you could ask yourself, which values those types can have. But we deliberately omit the runtime values of all types because the \emph{complete} differentiation is done at type-level in compile time. The exact values are more or less irrelevant for the implementation logic. A type fully encodes a function and should be seen as some kind of value for now.

In line 3 we apply the product rule by using the matched subexpressions \lstinline{l} and \lstinline{r} and recursively using \lstinline{D} on them. Addition works analogously but uses the sum rule.

Line 7 and 9 still look suspicious. What are integers doing as a type parameter? These are compile time value types which are now supported in Scala 3:
\begin{lstlisting}
type V[C <: Double | Int] <: Term
\end{lstlisting}
\lstinline{C} is a \lstinline{Double} or \lstinline{Int} singleton value type. By using \lstinline{scala.compiletime.constValue[C]} we can extract the singleton value from a singleton type. With this we can encode (integer) numbers fully at type-level and even calculate with them, also at type-level in compile time. Using this and our differentiation rules for the variable we can match the variable (which is the type \lstinline{X}) in line 6 and return a type which encodes the number 1. We do the same for constants where we match any constant (denoted by ``\_'') and differentiate it to 0.

If we combine all functionalities described above we gain the possibility to define a function and produce its derivative entirely at type level:
\begin{lstlisting}
type F = X * X
type DF = D[F] // -> X * D[X] + D[X] * X -> X * V[1] + V[1] * X
// the "symbolic" differentiation is already completed here (at compile time)

// the following is necessary to compute an actual result from the derivative (at runtime)
val df: DF = initFromType[DF] 
val result: Double = df(3)
\end{lstlisting}
The type \lstinline{F} (line 1) is the function we want to differentiate with respect to \lstinline{X}. As mentioned before it does \emph{not} need a value. We use our match type \lstinline{D} to produce the new type \lstinline{DF} which is equivalent to \lstinline{X * V[1] + V[1] * X}. That is, because it matches \lstinline{F} to be multiplication and uses the product rule. At this point the differentiation is completed and \lstinline{DF} fully represents the differentiated function.

Unfortunately computing a result at type level is impossible because we want to support decimal numbers. Type level calculation with integers on the other hand would be very possible and is included in Scala 3. To allow decimals we have to use a function \lstinline{initFromType} which recursively matches the provided type and constructs a function value which computes the result later at runtime. Essentially this means that we still have to fall back to actual values at runtime if we want usable results from our implementation and ca not pull through a full implementation at type-level and in compile time.

Additionally, we had a major problems concerning compilation time. Multiple chained multiplications took minutes to compile. At first, we blamed our implementation because we shifted (almost) all of our logic into compile time. An update of the Scala compiler to the newest version (3.0.2 as of now) solved that problem. Compilation time now takes no longer than usual compilations.
