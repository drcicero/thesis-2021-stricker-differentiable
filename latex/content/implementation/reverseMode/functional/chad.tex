\subsection{Combinatory Homomorphic Automatic Differentiation (CHAD)} \label{sec:chad}

Until now we relied on a map to accumulate all adjoints of all expressions by mapping \lstinline{Num} to \lstinline{Double} to get rid of mutation. From a functional programming standpoint this is still a compromise. We mapped specific instances of \lstinline{Num} which means we have to generate an unique ID for every object to get a functioning map. In contrast imagine we would build our map by strictly comparing values (i.e. instances of \lstinline{Num} with the same numeric value are treated as indistinguishable). Sub-expressions of our calculation with the same result value would share one adjoint which is clearly flawed. To have a purely functional implementation we have to get rid of instance comparison which means to drop the map. We also take one step back from monads and return to dual numbers:
\begin{lstlisting}
case class Dual(v: Double, variableAdjoint: Double => Double):
    def *(that: Dual): Dual =
        def variableAdjointBothSides(
            partialAdjointThis: Double, 
            partialAdjointThat: Double
        ) =
            this.variableAdjoint(partialAdjointThis) 
                + that.variableAdjoint(partialAdjointThat)

        def partialAdjointThis(parentAdjoint: Double) = 
            parentAdjoint * that.v
            
        def partialAdjointThat(parentAdjoint: Double) = 
            parentAdjoint * this.v

        Dual(
            this.v * that.v,
            parentAdjoint =>
                variableAdjointBothSides(
                    partialAdjointThis(parentAdjoint), 
                    partialAdjointThat(parentAdjoint)
                )
        )
    end *

    def +(that: Dual): Dual = ???
\end{lstlisting}
First thing to notice in line 1 is that \lstinline{Dual} now has a member named \lstinline{variableAdjoint}. It calculates an adjoint using the parent adjoint like already familiar. But the important difference (and the reason for the prefix ``variable-'') is that it does \emph{not} compute the partial adjoint of the current expression. It instead accumulates the sum of all partial adjoints of our variable (i.e. x). It ignores all partial adjoints of expressions not including x. We don't need them anyway. 

Lines 10 to 14 look very familiar because it just computes the partial adjoints of \lstinline{this} and \lstinline{that}. From line 15 on we create a new \lstinline{Dual} with a new \lstinline{variableAdjoint}.  We calculate both partial adjoints (lines 20 and 21) and then pass it to \lstinline{variableAdjointBothSides}. \lstinline{variableAdjointBothSides}'s job is to sum the partial adjoints of x from both branches but has to ensure that only relevant partial adjoints are included and for example constants are ignored. The function does in fact just sum both variable adjoints together (lines 7 and 8). So where do we ensure that only relevant partial adjoints are summed? The answer lies in how constants and the variable are instantiated/defined:
\begin{lstlisting}
def variable(v: Double): Dual = Dual(v, identity)
def const(v: Double): Dual = Dual(v, _ => 0.0)
\end{lstlisting}
The $\diff{x}{x} = 1$ partial adjoint of x is just directly its partial adjoint without further ado. This translates to the variable having \lstinline{identity} as its variable adjoint function (line 1). The variable adjoint of constants are always zero (line 2). This makes sense because constants have no meaningful partial adjoint for our variable. Effectively when \lstinline{variableAdjointBothSides} calls \lstinline{this.variableAdjoint(partialAdjointThis)} and \lstinline{this} is the variable x then it evaluates to \lstinline{partialAdjointThis}. If \lstinline{this} is a constant the call evaluates to zero. If \lstinline{this} is neither (e.g. a multiplication expression) then the previous two rules are implicitly applied for all branches of that expression recursively. This, after some consideration, rather simple design is sufficient to implement reverse mode differentiation fully functionally and without building some kind of virtual stack/tape as we did previously.

Another advantage of this approach is its simplicity for the user:
\begin{lstlisting}
given Conversion[Double, Dual] = const(_)
given Conversion[Int, Dual] = const(_)

def f(xDouble: Double): Dual =
    val x: Dual = variable(xDouble)
    2 * x + x * x * x
end f

val derivative = f(3).variableAdjoint(1)
\end{lstlisting}
After declaring the variable explicitly in line 5 (and using given conversions (lines 1 and 2) for constants) we can write \lstinline{f} simply like an usual arithmetic expression in Scala (line 6). We don't even need a \lstinline{differentiate} function because we accumulate the correct adjoint of x on the fly while calculating the result of \lstinline{f}. In previous reverse mode implementations we called \lstinline{adjoint} on x to get its adjoint which makes sense from a mathematical standpoint because we are interested in the adjoint of our variable and nothing else. In this case however we have to call \lstinline{variableAdjoint} on the result of \lstinline{f}. Ultimately we obviously still get the adjoint of x but the semantics of \lstinline{variableAdjoint} are different (as the name suggests). It can be most easily interpreted as the sum of all partial adjoints of x contained in all branches of the current expression. Therefore we must call it on the outermost expression to include all partial adjoints of x in our whole calculation.