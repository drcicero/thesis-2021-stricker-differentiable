\section{Continuation Passing Style (CPS)}

\todocite{Lantern paper}

For our first reverse mode attempt \todowording we want to build on already implemented and understood code, i.e. our dual number implementation from \reflst{lst:lst:forwardDualNumber}. A very similar structure can be achieved by using continuation passing style (CPS). This is just a fancy term \todowording for the frequently used callbacks (e.g. for frontend web development) \todocite{?}. Essentially you pass the ``rest of the calculation'' to the function instead of using its return value and manually applying the ``rest'' on that result. To make things clear, \todopunctuation consider chaining two arbitrary functions (with unspecified types \lstinline{A, B, C, D}) as usual:
\todo{Add "composed" function of lines 11-13 (in both examples)}
\begin{lstlisting}
def first(x: A): B = ???
def second(x: B): D = ???

val a: A = ???
val firstResult: B = first(a)
val secondResult: D = second(firstResult)
\end{lstlisting}
And now an equal implementation but with continuations:
\begin{lstlisting}
def first[R](x: A)(rest: B => R): R = ???
def second[R](x: B)(rest: D => R): R = ???

val a: A = ???
val secondResult: D = 
    first(a) { firstResult => 
        second(firstResult) { identity }
    }
\end{lstlisting}
Notice that the input type of \lstinline{rest} in line 1 is \lstinline{B} which matches the result type of \lstinline{first} from the ordinary example above (and analogously for \lstinline{D} and \lstinline{second}). To show the general equality of both approaches we also introduced type parameter \lstinline{R} to both functions. This is needed to support arbitrary ``second'' functions even if they do not return exactly \lstinline{D}. In line 9 we pass \lstinline{identity} to \lstinline{second} to mark the ``end'' of the calculation. If we had a third operation, we would pass a further nested lambda which calls \lstinline{third} (and so on for \lstinline{fourth}, \lstinline{fifth}, ...). When following CPS strictly, every function takes a continuation. and ordinary variables are never used. Lambdas with named parameters fullfil that role instead (as seen with \lstinline{firstResult} above).
\todo{Add theoretical explanation of code above (\url{https://pbs.twimg.com/media/FAjvTXRXEAwKuT-?format=png&name=small})}

Using this, at first sight rather obscure, feature we can implement reverse mode very similar to \reflst{lst:lst:forwardDualNumber}:
\begin{lstlisting}[mathescape=true]
case class Dual(x: Double, var adjoint: Double):
    def *(that: Dual)(k: Dual => Dual): Dual =
        // Compute result of current operation and init adjoint with 0
        val localResult = Dual(this.x * that.x, 0) 

        // "wait" for all remaining (nested) calculations to finish which implicitly calculate the adjoint of the current operation (localResult.adjoint)
        val globalResult = k(localResult)

        // We can now compute the partial adjoint of the current computation branch for "this" and "that" respectively. If "this" or "that" occur in other computation branches then that brach is responsible for adding its partial adjoint.

        // += $\overw{\text{this}}^z$
        this.adjoint +=
            // $\overw{\text{this}}^z$
            that.x // $\diff{w_p}{w_{\text{this}}^z}$
                * localResult.adjoint // $\overw{p}$

        // += $\overw{\text{that}}^z$
        that.adjoint +=
            this.x // $\diff{w_p}{w_{\text{that}}^z}$
                * localResult.adjoint  // $\overw{p}$

        globalResult
    end *

    // Analogous to (*)
    def +(that: Dual)(k: Dual => Dual): Dual =
        val localResult = Dual(this.x + that.x, 0)
        val globalResult = k(localResult)
        this.adjoint +=
            1 // $\diff{w_p}{w_{\text{this}}^z}$
                * localResult.adjoint // $\overw{p}$
        that.adjoint +=
            1 // $\diff{w_p}{w_{\text{that}}^z}$
                * localResult.adjoint // $\overw{p}$
        globalResult
    end +
end Dual

def differentiate(f: Dual => (Dual => Dual) => Dual)(x: Double): Double = {
    val xDual = Dual(x, 0)

    // The result of f does not interest us directly. We only need its side effects
    f(xDual) { topExpression => {
        // We have to mark the top-most expression manually because our program would have no way to recognize it
        topExpression.adjoint = 1 // $\diff{y}{y}$
        topExpression
    }
    }
    xDual.adjoint // $\overline{x} = \diff{y}{x}$
}

def f(x: Dual)(k: Dual => Dual): Dual =
    // 2 * x + x * x * x
    (2 * x) { y1 =>
        (x * x) { y2 =>
            (y2 * x) { y3 =>
                (y1 + y3) { k }
            }
        }
    }
end f

val derivative: Double = differentiate(f)(3)

\end{lstlisting}

