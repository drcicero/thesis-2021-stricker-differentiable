\subsection{Continuation Passing Style (CPS)} \label{sec:cps}

Continuation is just an elaborate term for the frequently used callbacks (e.g.\ for frontend web development). Essentially you pass the ``rest of the calculation'' to the function instead of using its return value and manually applying the rest on that result. To make things clear consider chaining two arbitrary functions (with unspecified types \lstinline{A, B, C, D}) as usual on the left-hand side and an equal implementation but in CPS on the right-hand side:
\\
\begin{minipage}[c]{0.45\textwidth}
\begin{lstlisting}[caption={Ordinary chaining}, label={lst:ordinaryChaining}]
def first(x: A): B
def second(x: B): D

def chained(x: A): D =
  val firstResult: B = first(a)
  return second(firstResult)

val a: A = ???
val chainedResult: D =
  chained(a)
\end{lstlisting}
\end{minipage}%%
\begin{minipage}[c]{0.55\textwidth}
\begin{lstlisting}[caption={CPS chaining}, label={lst:cpsChaining}]
def first[R](x: A)(rest: B => R): R
def second[R](x: B)(rest: D => R): R

def chained[R](x: A)(ret: D => R): R =
  first(a) { (firstResult: B) =>
    second(firstResult) { ret }
  }
val a: A = ???
val chainedResult: D =
  chained(a) { identity }
\end{lstlisting}
\end{minipage}
\\
\\
On the left-hand side we first define two functions which have unspecified implementations. The types are the important part. \lstinline{first} takes \lstinline{A} and returns \lstinline{B}. \lstinline{B} is in turn the input type of \lstinline{second} which is important because this makes both functions chainable. \lstinline{chained} (line 4) calls \lstinline{first} (line 5) and passes its result to \lstinline{second} (line 6) which is essentially the definition of function chaining. Lines 8 to 10 just visualize how \lstinline{chained} is then used.

The right-hand side looks somewhat similar but has significant changes. Every function now has a second argument, i.e.\ the continuation which we usually call \lstinline{rest}. Notice that the \emph{input} type of \lstinline{rest} in \lstinline{first} of \reflst{lst:cpsChaining} is \lstinline{B}. This matches the \emph{result} type of \lstinline{first} in \reflst{lst:ordinaryChaining} (and analogously for \lstinline{D} and \lstinline{second}). Also note that we had to introduce type parameter \lstinline{R} to all functions. This is needed to support arbitrary \lstinline{rest} functions even if they do not return exactly \lstinline{D}. \lstinline{chained} works similarly but looks very different. We also call \lstinline{first} in line 5 but instead of creating a new named (constant) variable we have to pass a lambda where the parameter takes the role of the variable. After that we call \lstinline{second} and pass \lstinline{ret} as \lstinline{rest} which in this case has an equal semantic to the \lstinline{return} statement in line 6 of the left-hand side. In line 10 of \reflst{lst:cpsChaining} we pass \lstinline{identity} to \lstinline{chained} to mark the end of the calculation because it acts like a no-op. We could have passed another arbitrary operation instead, similarly to how we could have applied more arbitrary operations on \lstinline{chainedResult} in \reflst{lst:ordinaryChaining}. When following CPS strictly, every function takes a continuation and ordinary variables are never used. Lambdas with named parameters fulfill that role instead, as seen with \lstinline{firstResult} in line 5.

Using CPS which is an at first glance rather obscure feature we can implement reverse mode using dual numbers:
\begin{lstlisting}[mathescape=true, caption={Reverse mode CPS}, label={lst:cpsDual}]
case class Dual(x: Double, var adjoint: Double):
    def *(that: Dual)(k: Dual => Dual): Dual =
        // $ w_p $
        val localResult = Dual(this.x * that.x, 0)

        val globalResult = k(localResult)
        
        // $ i \in \{ \text{this}, \text{that} \} $
        def addPartialAdjoint(
            thisOrThat: Dual, 
            derivativeWrtThisOrThat: Double
        ): Unit =
            // $ \overw{i}^z = \overw{p} \diff{w_p}{w_i^z} $ 
            val partialAdjoint = 
                localResult.adjoint * derivativeWrtThisOrThat
            // $ \overw{i} \pluseq \overw{i}^z $
            thisOrThat.adjoint += partialAdjoint
        end addPartialAdjoint

        addPartialAdjoint(this, that.x) // $ i = \text{this} $
        addPartialAdjoint(that, this.x) // $ i = \text{that} $
        globalResult
    end *

    // Analogous to (*)
    def +(that: Dual)(k: Dual => Dual): Dual = ???
end Dual
\end{lstlisting}
Compared to forward dual numbers (\reflst{lst:forwardDualNumber}) we changed the name of the second member of \lstinline{Dual} to \lstinline{adjoint} to reflect the shift in focus from $\dot w_i$ to $\overw{i}$. The comments add translations from code expressions into their mathematic notation from \refsec{sec:reverseMode}. $w_p$ in line 13 signifies the parent expression, e.g. for multiplication we would write it like this:
\begin{align*}
    w_{\text{this}} &= \text{\lstinline{this}} \\
    w_{\text{that}} &= \text{\lstinline{that}} \\
    w_p &= w_{\text{this}} \cdot w_{\text{that}}
\end{align*}
The helper function \lstinline{addPartialAdjoint} (line 9) essentially just executes the mathematical expressions in lines 13 and 16 but generalizes over for which subexpression ($\overw{\text{this}}^z$ or $\overw{\text{that}}^z$) to compute the partial adjoint. The \lstinline{partialAdjoint} (line 14) is the adjoint of this specific occurrence of $ w_i^z $. Remember that we have to sum the partial adjoints of all occurrence to get the full adjoint. Exactly that happens in line 17 by mutating \lstinline{thisOrThat.adjoint}. Every expression is responsible to add the partial adjoints of their subexpressions. By translating every code piece into its corresponding mathematical notation, one can clearly see the close relationship between our implementation and the mathematical foundations of reverse mode differentiation seen in \refsec{sec:reverseMode}.

The main difference is line 6 where we call the continuation \lstinline{k}. It represents the rest of the computation as stated previously. In this context specifically, it represents all further operations that might use the passed \lstinline{localResult}. Note that those further operations are ancestors and not children or in other words they are outer and not inner operations. The first job of the continuation is to do a \emph{forward pass} through the rest of the operations. This is done by calculating the regular result (line 4) of the next operation and afterwards calling the next continuation. This recursive forward pass eventually finishes by calling a \lstinline{k} which does not call another continuation. At this point we found the recursion anchor and have built a stack of calls as usual with recursive algorithms. This built-up stack now naturally tears down in \emph{reverse order}. This is exactly our primary goal. We had to calculate the regular results in ``normal'' order (i.e. inner to outer expression) but the adjoint is naturally calculated in \emph{reverse order} (as seen in \reffig{fig:informationFlow}). The built-up stack is visualized in \reffig{fig:stack} for our running example. It tears down from top to bottom.
\begin{center}
    \tikzset{
		every node/.style={draw, text height=1.5ex},
		split/.style={rectangle split, rectangle split parts=#1,draw,
			rectangle split horizontal=false,rectangle split part align=base},
	}

    \begin{tikzpicture}[->, -{Latex}]
        \node[split=5] (a2) at (4,1) {\nodepart{one} $w_4\ (+)$ \nodepart{two} $w_1\ (x_1)$ \nodepart{three} $w_3 (*)$ \nodepart{four} $w_1 (x_1)$ \nodepart{five} $w_2 (x_2)$};
    \end{tikzpicture}

    \captionsetup{type=figure}
    \caption{Expression stack after the forward pass}
    \label{fig:stack}
\end{center}
From here on therefore the \emph{reverse pass} starts. On top of the stack now resides $w_4$. Lines 9 to 20 are executed to update the adjoint of its subexpressions ($w_1$ and $w_3$). After that, $w_1$ and then $w_3$ and its whole expression tree on the stack is handled. As you can see, after doing the forward pass by abusing continuations we now iterate through each expression in the same order as we would do when doing reverse differentiation by hand. Essentially we have linearized the expression tree to make the reverse pass and therefore adjoint accumulation easier. We use some kind of linearization in almost every reverse mode implementation.

In the end we define a \lstinline{differentiate} operator and call it:
\begin{lstlisting}[mathescape=true]
def differentiate(f: Dual => (Dual => Dual) => Dual)(x: Double): Double =
    val xDual = Dual(x, 0)

    // Use only side effects
    f(xDual) { topExpression => {
        // Manually set adjoint of top-most expression
        topExpression.adjoint = 1 // $\overline{y} = \diff{y}{y}$
        topExpression // $y$
    }
    }
    xDual.adjoint // $\overline{x} = \diff{y}{x}$
end differentiate

def f(x: Dual)(k: Dual => Dual): Dual =
    // 2 * x + x * x * x
    (Dual(2, 0) * x) { y1 =>
        (x * x) { y2 =>
            (y2 * x) { y3 =>
                (y1 + y3) { k }
            }
        }
    }
end f

val derivative: Double = differentiate(f)(3)
\end{lstlisting}
\lstinline{differentiate} in line 1 takes the function \lstinline{f} we want to differentiate as its first argument. Because we follow CPS \lstinline{f} has two inputs, first the input for \lstinline{x} of type \lstinline{Dual} and second the continuation of type \lstinline{(Dual => Dual)} which is called by \lstinline{f} itself after calculating its result. The result of the continuation is the final result of type \lstinline{Dual}. \lstinline{differentiate} also gets the \lstinline{Double} value passed at which we want to differentiate \lstinline{f}. In line 2 we create a \lstinline{Dual} from \lstinline{x}. Its initial adjoint has to be zero because its partial adjoints will get added to it by mutation later. The continuation passed to \lstinline{f} in line 5 is basically just an identity function to mark the top most expression and to act as the recursion anchor. We just have to additionally set the adjoint of the top expression to 1 because our program would not know which the top expression is. Another point to notice is that we do not directly use the result of \lstinline{f} and instead read the mutated adjoint of $x$ in line 11. This makes sense because \lstinline{f} returns the result of the \emph{top} expression. Its adjoint is trivially 1 (line 7) and therefore is not interesting while the adjoint of $x$ (line 11) is exactly the derivative of \lstinline{f}.

The biggest disadvantage of this CPS implementation is how one has to write the function \lstinline{f} compared to for example our forward mode dual number implementation in \reflst{lst:forwardDualNumberDiff}. Reading CPS is not impossible because one ``just'' has to read every line from right to left. For example consider line 17. On the right-hand side is the variable name (\lstinline{y2}) and on the left-hand side its ``value'' (\lstinline{x * x}). We also have to give every subexpression a name which we would normally not need to and also often do not want to. Another problem is the deep nesting which occurs for more elaborate functions. A CPS function is therefore cumbersome to write and reading them needs some time getting used to. Those problems could be solved by using shift and reset operators which principally would make continuations implicit and would hide them completely from client code. Unfortunately there is currently no maintained implementation of them for Scala. Hence, we would like to refer to Fei Wang et al. \cite{lantern} and their implementation of reverse mode differentiation using shift and reset operators without further going into it here.
