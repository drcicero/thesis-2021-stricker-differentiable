\subsection{Tape} \label{sec:tape}

The following implementation is very similar to CPS but instead of building a stack of calls implicitly by calling continuations we build that ``call stack'' manually. Remember that the only goal we achieved by using continuations was a two pass design which we used to do some operations (compute regular result) in normal order and some operations (compute adjoint) in reverse order through the expression tree. Another way to achieve this is to do the forward pass as usual but on the way additionally save all operations which have to be done in the reverse pass for later. When we have collected every operation we just execute them in ``reverse'' order:
\begin{lstlisting}[mathescape=true]
var tape: Unit => Unit = _ => ()

case class Dual(x: Double, var adjoint: Double):
    def *(that: Dual): Dual =
        val localResult = Dual(this.x * that.x, 0)

        def addPartialAdjoint(
            thisOrThat: Dual,
            derivativeWrtThisOrThat: Double
        ): Unit => Unit =
            _ =>
                val partialAdjoint = 
                    localResult.adjoint * derivativeWrtThisOrThat
                thisOrThat.adjoint += partialAdjoint
        end addPartialAdjoint

        tape = addPartialAdjoint(this, that.x) andThen tape
        tape = addPartialAdjoint(that, this.x) andThen tape

        localResult
    end *

    def +(that: Dual): Dual = ???
end Dual
\end{lstlisting}
In line 1 we define a mutable \lstinline{tape} which we use to store operations on. These operations can only produce side effects because the \lstinline{tape} has type \lstinline{Unit => Unit} which cannot take nor return anything meaningful. We initialize it with a no-op.
The first part of the multiplication (lines 5 to 15) which includes \lstinline{addPartialAdjoint} are in essence equal to the according lines in CPS, and therefore we will just highlight the differences. We also omitted the mathematical translations. They are still important to get the connection to the mathematical foundations but for them refer to \reflst{lst:cpsDual} as they are very similar.

First thing to note is the altered return type of \lstinline{addPartialAdjoint} (line 10). It now returns a function which in turn is just used for its side effects (\lstinline{Unit => Unit}). This means that when we call \lstinline{addPartialAdjoint} (lines 17 and 18) the adjoint is \emph{not} directly updated opposed to CPS. Instead, we prepend that ``operation'' (calculating and updating the adjoint of \lstinline{this} or \lstinline{that}) to \lstinline{tape}. We prepend (instead of appending) so that in the end we have a tape which executes each operation in reverse order of insertion.

Our \lstinline{differentiate} function is again similar to CPS:
\begin{lstlisting}
def differentiate(f: Dual => Dual)(x: Double): Double =
    tape = _ => ()
    val xDual: Dual = Dual(x, 0)
    val topExpression = f(xDual)
    topExpression.adjoint = 1
    tape(())
    xDual.adjoint
end differentiate

def f(x: Dual): Dual =
    Dual(2, 0) * x + x * x * x

val derivative = differentiate(f)(3)
\end{lstlisting}
This time \lstinline{differentiate} takes a simpler \lstinline{f} as its first argument because we do not use continuations anymore. It now just has an \lstinline{Dual} input and calculates a \lstinline{Dual}. Because \lstinline{tape} is a global variable we have to remember to reset it for every differentiation (line 2). We then call \lstinline{f} (line 4) to do the forward pass and to populate the \lstinline{tape}. Similar to CPS we have to manually set the adjoint of the top expression to $1 = \diff{y}{y}$ (line 5). At this point no differentiation has been done yet. We have to call \lstinline{tape} to start it manually (as it takes a \lstinline{Unit} we have to pass its only inhabitant, namely ``\lstinline{()}''). The definition of \lstinline{f} (line 10 and 11) is possibly the most interesting change. We do not need any continuations and can omit variable names which makes it easier to read and write.

To make the definition of \lstinline{f} even more regular we can define an implicit conversion which converts a constant into \lstinline{Dual} automatically. For this we use \lstinline{given} instances~\cite{givensScala3} of \lstinline{Conversion}~\cite{conversionsScala3} which were introduced in Scala 3. They specifically describe the intent to convert a value. Previously \lstinline{implicit} methods were used for this but their semantics were overloaded and for example have also been used to define extension methods. The first \lstinline{given} instance (line 1) is not needed for this example but is included for completeness if one uses decimal numbers:
\begin{lstlisting}
given Conversion[Double, Dual] = Dual(_, 0)
given Conversion[Int, Dual] = Dual(_, 0)

def f(x: Dual): Dual =
    // 2 is implicitly converted into Dual(2, 0)
    2 * x + x * x * x
\end{lstlisting}

So far we have only done reverse mode differentiation for one variable. As mentioned previously reverse mode differentiation shines when having multiple input variables. Therefore, it's apparent to make an example which supports that. Extending the tape implementation to take multiple variables is mostly trivial as we only have to change the \lstinline{differentiate} function:
\begin{lstlisting}
def differentiate(
    f: List[Dual] => Dual, 
    xs: List[Double]
): List[Double] =
    tape = _ => ()
    val xsDual: List[Dual] = xs map { Dual(_, 0) }
    f(xsDual).adjoint = 1
    tape(())
    xsDual map { _.adjoint }
end differentiate

def f(xs: List[Dual]): Dual =
    2 * xs(0) + xs(1) * xs(2) * xs(2)

val derivatives: List[Double] = differentiate(f, List(3.0, 5.0, 2.0))
\end{lstlisting}
We encode multiple variables as a single vector of type \lstinline{List[Dual]}. Because \lstinline{f} now takes a \lstinline{List[Dual]}, \lstinline{differentiate} has to reflect that by accepting a function \lstinline{List[Dual] => Dual} and a vector of values to differentiate \lstinline{f} at. At first, we have to reset the tape again (line 5). In line 6 we extract the \lstinline{adjoint} of each variable which ultimately gives us a vector where every value is the derivative with respect to one variable. In other words we computed the gradient of \lstinline{f}. The main takeaway here is that we computed the derivative of multiple variables in one go without having to call \lstinline{differentiate} multiple times with different values. This is only possible with reverse mode and is its main advantage. Forward mode would have to do one full differentiation for each variable where all other variable are set to 0.

Extending other reverse mode implementations for multidimensional functions (in input or output) is done analogously. To allow better focus on the essential differences and keep the examples simple we mostly concentrate on single dimensional functions from here on.